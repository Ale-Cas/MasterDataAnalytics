{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this exam you will be reporting on your implementation of the three main regularization strategies introduced in class, namely Ivano, Tikhonov regularization and early stopping (or computational regularization).\n",
    "\n",
    "\n",
    "### The plan\n",
    "\n",
    "You will work exclusively with linear models and the squared loss. Rather than using libraries to fit your models however, you will implemet the learning and testing algorithms from scratch.\n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Instructions\n",
    "\n",
    "Throughout this exam there will be some cells that you will be required to fill in. Complete with the missing code and return the filled notebook on Teams\n",
    "Some problems will ask you to comment a bit on the results that you will be plotting. In those cases, write in the corresponding markdown cell everything you think it's useful to show that you understood what you have done in that cell.\n",
    "\n",
    "Best wishes!\n",
    "\n",
    "### Packages\n",
    "\n",
    "We will be using standard packages for ML in python such as `numpy` for all linear-algebra related operations and `matplotlib` for visualization. You can install them via [`pip`](https://packaging.python.org/en/latest/tutorials/installing-packages/) or [`anaconda`](https://anaconda.org/) as you prefer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear Models\n",
    "\n",
    "What is a linear model? It is just a function $f:\\mathbb{R}^d\\to\n",
    "\\mathbb{R}$ that can be represented by a vector $w\\in\\mathbb{R}^d$.\n",
    "\n",
    "In other words, for any input point $x \\in \\mathbb{R}^d$, evaluating $f$ in $x$ gives us the output\n",
    "\n",
    "$\\qquad f(x) = x^\\top w = x^1 w^1 + x^2 w^2 + \\dots + x^d w^d =\n",
    "\\sum_{i=1}^d x^i w^i$,\n",
    "\n",
    "where $w^i$ and $x^i$ represent the $i$-th components of the vectors $w$ and $x$ respectively.\n",
    "\n",
    "In other words, if $x$ is a vector (= a collection) of features describing our input, then $f(x)$ returns a weighted sum of such features.\n",
    "\n",
    "**Note:** in the following we assume that vectors are vertical, namely\n",
    "\n",
    "$x = \\left(\\begin{matrix}x^1 \\\\ \\vdots \\\\ x^d \\end{matrix}\\right)$\n",
    "\n",
    "and denote $x^\\top = (x^1,\\dots,x^d)$ its transpose as an horizontal vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "### Implementing Linear Models\n",
    "\n",
    "Let us implement the operation $w^\\top x$ in python. To do so, we need first to import the numpy library, which allows us to operate vectors easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then, we can set our vector $w$. Let's start with something simple in 2 dimensions:\n",
    "\n",
    "$w = (0.5, 0.3) \\in \\mathbb{R}^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w = np.array([0.5,0.3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us consider an input point $x$, let's say $x = (0.1, -0.4)$, the operation $f(x) = x^\\top w$ is now performed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define x\n",
    "x = np.array([0.1,-0.4])\n",
    "\n",
    "# f(x) = w^T x\n",
    "y = x[0]*w[0] + x[1]*w[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can print out the value of our output $y$ as follows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.06999999999999999\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The vector product $w^\\top x$ is an essential operation in linear algebra (an in machine learning as well!). An efficient implementation is available within the Numpy library. So we can alternatively write it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.06999999999999999\n"
     ]
    }
   ],
   "source": [
    "ynp = x.dot(w)\n",
    "\n",
    "print(ynp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We see that `ypn` is identical to `y`. The advantage is that the `dot` operation can be used with vectors of arbitrary size and it does not require us to write all the sums for all the products of the elements $w_i x_i$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Multiple evaluations\n",
    "\n",
    "Thanks to the lineartity of our model $f$, we have multiple inputs $x_1, x_2, \\dots, x_n$ (we used the subscript to denote different inputs), we can perform the evaluation of all $f(x_1), \\dots , f(x_m)$ simultaneously via a single matrix-vector product. To do this, we put all our inputs as the rows of a matrix $X\\in\\mathbb{R}^{n \\times d}$ such that\n",
    "\n",
    "$\\qquad X = \\left[\\begin{matrix}x_1^\\top \\\\ \\vdots \\\\ x_n^\\top \\end{matrix}\\right] = \\left[\\begin{matrix}x_1^1 & x_1^2  & \\dots & x_1^d \\\\ x_2^1 & x_2^2  & \\dots & x_2^d \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ x_n^1 & x_n^2  & \\dots & x_n^d\\end{matrix}\\right]$\n",
    "\n",
    "Then, we note that we can collect all the outputs $f(X) = (f(x_1), \\dots , f(x_m))^\\top$ in a single vector by performing the matrix vector product between $X$ and $w$, namely\n",
    "\n",
    "\n",
    "$\\qquad f(X) = X w = \\left[\\begin{matrix}x_1^\\top \\\\ \\vdots \\\\ x_n^\\top \\end{matrix}\\right] w = \\left[\\begin{matrix}x_1^\\top w \\\\ \\vdots \\\\ x_n^\\top w \\end{matrix}\\right] = \\left(\\begin{matrix}f(x_1) \\\\ \\vdots \\\\ f(x_n) \\end{matrix}\\right)$\n",
    "\n",
    "Luckily, we can carry out the matrix operation using the same notation as for the vector-vector product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11  0.18 -0.84]\n"
     ]
    }
   ],
   "source": [
    "x1 = np.array([0.1, 0.2])\n",
    "x2 = np.array([-0.3, 1.1])\n",
    "x3 = np.array([-2.1, 0.7])\n",
    "\n",
    "X = np.array([x1,x2,x3])\n",
    "\n",
    "f = X.dot(w)\n",
    "\n",
    "print(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluating the Empirical Risk\n",
    "\n",
    "Given the tools we have introduced, we can now proceed evaluating a loss function for a model $w$ on a certain dataset. Let $D = (x_i,y_i)_{i=1}^n$ a dataset of input output pairs (we assume $x_i$ to be vectors in $\\mathbb{R}^d$ and $y_i$ scalar values in $\\mathbb{R}$). Let $\\ell$ be a loss function that compares two outputs $f(x)$ and $y$ telling us that we pay $\\ell(f(x),y)$ in predicting $f(x) = x^\\top w$ instead of $y$. We can compute the **Empirical risk** of $w$ as the sum of all the errors $\\ell(x_i^\\top w, y_i)$:\n",
    "\n",
    "$\\qquad \\mathcal{E}(w) = \\ell(x_1^\\top w, y_1) + \\dots + \\ell(x_n^\\top w, y_n) = \\frac{1}{n} \\sum_{i=1}^n \\ell(x_i^\\top w, y_i)$\n",
    "\n",
    "For the rest of this notebook we will assume to work with the **squared loss**, namely\n",
    "\n",
    "$\\qquad \\ell(y',y) = (y' - y)^2$\n",
    "\n",
    "### **Problem 1**: given the dataset in the next cell, evaluate the empirical risk of $w = (0.5,0.3)$.\n",
    "\n",
    "*Hint:* you can write the empirical risk un closed form as\n",
    "\n",
    "$\\qquad\\mathcal{E}(w) = \\|Xw - y\\|^2$\n",
    "\n",
    "where $X$ is the matrix with input points as rows, $y$ is the vector with entries corresponding to the output points and $\\|\\cdot\\|$ denoting the (Euclidean) norm of a vector, namely\n",
    "\n",
    "$\\qquad \\|x\\|^2 = \\sum_{j=1}^d (x^j)^2$\n",
    "\n",
    "You can find all the operations you need from the documentation of `numpy`. For instance, you can compute the norm of a vector following the instructions [here](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (346688528.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [21]\u001b[0;36m\u001b[0m\n\u001b[0;31m    risk = # fill here\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "x1 = np.array([0.1, 0.2])\n",
    "x2 = np.array([-0.3, 1.1])\n",
    "x3 = np.array([-2.1, 0.7])\n",
    "\n",
    "X = [x1,x2,x3]\n",
    "\n",
    "y1 = np.array(0.1)\n",
    "y2 = np.array(3.0)\n",
    "y3 = np.array(-0.3)\n",
    "\n",
    "y = [y1,y2,y3]\n",
    "\n",
    "empirical_risk = # fill here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Making it a function\n",
    "\n",
    "We will need to compute the empirical risk of a model $w$ many times in the following. In these cases it is often useful to make the process you wrote above as a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def empirical_risk(w):\n",
    "    return # return the empirical risk computed in the previous cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Just remember that `empirical_risk` is computing the risk for $w$ but for the given dataset of three input-output points defined in the previous  cell. So if you want to change your dataset you will also need to change the function (you could do something smarter by passing the dataset to the function if you want. _Maybe there is a compact way to pass the dataset?_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optimiaztion: Minmizing the Empirical Risk\n",
    "\n",
    "One thing we might want to do, is to find the best model (or at least 'a' best model) $w$ that minimizes the empirical risk. In class we have called this the _empirical risk minimizer (ERM)_:\n",
    "\n",
    "\n",
    "$\\qquad w_{ERM} = \\arg \\min_{w\\in\\mathbb{R}^d} ~ \\mathcal{E}(w)$\n",
    "\n",
    "($\\arg\\min$ simply tries to find the _argument_ that _minimizes_ $\\mathcal{E}$).\n",
    "\n",
    "From what we have seen in class, the ERM might overfit our data. In the following we will review different form of regularization to avoid this problem), but for now let's just write an algorithm that minimizes $\\mathcal{E}(w)$. We will then adapt what we have implemented to include also regularization.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "In class we have seen how first order iterative optimization methods can be used to minimize an objective functional $\\mathcal{L}(\\cdot)$\n",
    "\n",
    "The gradient descent algorithm is implemented according to the following process:\n",
    "- choose an initial point $w_0$ (for example $w_0 = 0$).\n",
    "- for any time step $t>0$ we obtain $w_t$ as\n",
    "$\\qquad w_t = w_{t-1} - \\eta \\nabla \\mathcal{L}(w_{t-1}) \\qquad\\qquad$ with $\\eta>0$ a step size.\n",
    "Namely, from the previous iterate $w_{t-1}$, we perform a step in the opposite direction to the one pointed by the gradient, with step size $\\eta$.\n",
    "\n",
    "We can repeat this process for as long as we want, although usually we will set up a total number of desired steps $T$ and will  return $w_T$. Of course a key question is when is it a good idea to stop? Let's see what happens empirically:\n",
    "\n",
    "#### **Problem 2: Implementing Gradient Descent**\n",
    "\n",
    "Implement gradient descent to minimize the empirical risk. You will need to choose $\\eta$, $T$ and will need to be able to write down the gradient of $\\mathcal{E}(w)$. Regarding the latter, remember that:\n",
    "\n",
    "- The gradient of a sum is the sum of the gradients: $\\nabla \\mathcal{E}(w) = \\nabla \\frac{1}{n} \\sum_{i=1}^n (x_i^\\top w  - y_i)^2 =  \\frac{1}{n} \\sum_{i=1}^n \\nabla (x_i^\\top w  - y_i)^2$,\n",
    "- The gradient of the squared loss is\n",
    "$\\qquad\\qquad\\qquad \\nabla (x_i^\\top w  - y_i)^2 = 2 x_i x_i^\\top w + y_i x_i$\n",
    "\n",
    "__**Note:** analogously to when we computed the empirical risk $\\mathcal{E}(w)$ using a compact matrix notation (by putting all our input-output training points in the matrix $X$ or in the vector $y$) also the gradient $\\nabla \\mathcal{E}(w)$ can be written (and computed) in matrix notation using only matrix-matrix and matrix-vector products. Doing this is not necessary to succeed in this problem, but it can help you out speeding up computations__\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "T = 100\n",
    "eta = 1.0\n",
    "\n",
    "d = 2\n",
    "\n",
    "# set the initial w to be the vector with all 0s\n",
    "w = np.zeros(d)\n",
    "\n",
    "# in this list we will store the function values\n",
    "values = []\n",
    "\n",
    "# let's start by putting the first value in\n",
    "values.append(empirical_risk(w))\n",
    "\n",
    "# implement gradient descent\n",
    "for t in range(T):\n",
    "    grad_w = # compute the gradient of the empirical risk\n",
    "    w = # write the update rule\n",
    "\n",
    "    # compute the empirical risk of the current iterate and store it\n",
    "    values.append(empirical_risk(w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Plotting\n",
    "\n",
    "If you want to visualize what's going on with the function value across multiple iterations you can use the following code. It will be useful also in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mvalues\u001b[49m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'values' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Try a few different $\\eta$s to get a sense of what happens when they are too large, too small or \"just about right\". But in general... how do we choose $\\eta$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Convergence of Gradient Descent\n",
    "In class we have discussed how gradient descent \"converges\" to the ideal solution of the optimization problem when the objective functional $\\mathcal{L}(\\cdot)$ is convex and $M$-smooth (the latter means that the derivative $\\nabla\\mathcal{L}(\\cdot)$ is Lipschitz with constant $M>0$).\n",
    "\n",
    "We are in luck! The squared loss is:\n",
    "\n",
    "- Convex,\n",
    "- It's differentiable\n",
    "- It's gradient is Lipschitz continuous **(Question: with constant $M=?$)**\n",
    "\n",
    "The sum (or average) of multiple functions with the properties above still enjoy the same properties. Therefore, they hold also for the case $\\mathcal{L}(\\cdot) = \\mathcal{E}(\\cdot)$ the objective functional corresponds to the empirical risk.\n",
    "\n",
    "**Convergence**: A theorem that we have discussed in class tells us that as long as we take $\\eta \\in (0, 2/M)$, gradient descent converges. Verify this by yourself by taking $\\eta = 1/M$ in the next cell. In the case of the squared loss risk $M$ corresponds to the largest singular value of the covariance matrix $X^\\top X$. In numpy this can be done as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "T = 1000\n",
    "eta = 1.0 / np.linalg.norm(X.T.dot(X), 2)\n",
    "\n",
    "w = 0\n",
    "\n",
    "# in this list we will store the function values\n",
    "values = []\n",
    "\n",
    "# let's start by putting the first value in\n",
    "values.append(empirical_risk(w))\n",
    "\n",
    "# implement gradient descent\n",
    "for t in range(T):\n",
    "    grad_w = # compute the gradient of the empirical risk\n",
    "    w = # write the update rule\n",
    "\n",
    "    # compute the empirical risk of the current iterate and store it\n",
    "    values.append(empirical_risk(w))\n",
    "\n",
    "\n",
    "plt.plot(values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Overfitting and Regularization\n",
    "\n",
    "Now that we have implemented a startegy to find a potentially good model $w$, we should see how it fares on test data. To do so, we should repeat the whole process with real data however, so that training and test set are not arbitrary, but rather coming from the same distribution. Let's load a train and test dataset in matrix format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path_to_data = # put the string to the file 'dataset.pckl'\n",
    "\n",
    "f = open(path_to_data, 'rb')\n",
    "data = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "Xtr = data['Xtr']\n",
    "ytr = data['ytr']\n",
    "Xts = data['Xts']\n",
    "yts = data['yts']\n",
    "\n",
    "# get the actual dimension of the space where the inputs live!\n",
    "ntr, d =  Xtr.shape\n",
    "nts = Xtr.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have put our input and output training data points (those on which we will run our empirical risk minimization algorithm) inside the rows of the matrix `Xtr` and the entries of the vector `ytr`. The matrix `Xts` and `yts` contain the test data points.\n",
    "\n",
    "Let's repeat the training process finding the empirical risk minimizer on the training points.\n",
    "\n",
    "**Remember to change the `empirical_risk` function with the new dataset!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "T = 1000\n",
    "eta = # set it\n",
    "\n",
    "w = np.zeros(d)\n",
    "\n",
    "def empirical_risk(w):\n",
    "    return # compute the empirical risk of w on the training set Xtr, ytr\n",
    "\n",
    "# in this list we will store the function values\n",
    "values = []\n",
    "\n",
    "# let's start by putting the first value in\n",
    "values.append(empirical_risk(w))\n",
    "\n",
    "# implement gradient descent\n",
    "for t in range(T):\n",
    "    grad_w = # compute the gradient of the empirical risk\n",
    "    w = # write the update rule\n",
    "\n",
    "    # compute the empirical risk of the current iterate and store it\n",
    "    values.append(empirical_risk(w))\n",
    "\n",
    "\n",
    "plt.plot(values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally compute the test error and compare it with the final (and best) training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3065970656.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [23]\u001b[0;36m\u001b[0m\n\u001b[0;31m    test_error = # compute here the error of the model w on the test set.\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "test_error = # compute here the error of the model w on the test set Xts, yts\n",
    "\n",
    "print('Training error:', values[-1])\n",
    "print('Test error:', test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Are they comparable? If not, this is due to the model fitting too well the training data and not generalizing to test points, something that we referred to as **Overfitting** in class.\n",
    "\n",
    "Now that we have seen overfitting in action let us consider and implement a few of the regularization strategies we discussed in class.\n",
    "\n",
    "### Tikhnov Regularization\n",
    "\n",
    "In Tikhonov regularization we want to find a model $w$ that fits well our training data while not growing too \"complicated\". This is enforced by means of a penalty (or regularizer) $P(w)$ in the optimization problem\n",
    "\n",
    "$\\qquad w_\\lambda = \\arg\\min_{w} ~ \\mathcal{E}(w) + \\lambda P(w).$\n",
    "\n",
    "In particular, we considered the case where $P(w) = \\|w\\|^2$ is the squared Euclidean norm.\n",
    "\n",
    "#### **Problem 3**: Implementing Tikhonov Regularization\n",
    "\n",
    "Implement gradient descent for tikhonov regularization. Recall that the gradient of the squared Euclidean norm is\n",
    "\n",
    "$\\qquad \\nabla \\|w\\|^2 = 2 w$,\n",
    "\n",
    "and that the Lipschitz constant of the function $w\\mapsto \\lambda w$ is $\\lambda$ (this will be useful to choose $\\eta$ appropriately for gradient descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "T = 1000\n",
    "eta = # how to choose it?\n",
    "lam = # how to choose it?\n",
    "\n",
    "w = 0\n",
    "\n",
    "# in this list we will store the function values\n",
    "values = []\n",
    "\n",
    "# define a new function that compute the regularized empirical risk (namely the risk plus the regularization penalty)\n",
    "def regularized_empirical_risk(w, lam):\n",
    "    return # return the regularized empirical risk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# let's start by putting the first value in\n",
    "values.append(regularized_empirical_risk(w,lam))\n",
    "\n",
    "# implement gradient descent\n",
    "for t in range(T):\n",
    "    grad_w = # compute the gradient of the empirical risk\n",
    "    w = # write the update rule\n",
    "\n",
    "    # compute the empirical risk of the current iterate and store it\n",
    "    values.append(regularized_empirical_risk(w,lam))\n",
    "\n",
    "\n",
    "plt.plot(values)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As part of this problem, compute multiple the test error for the final model. Play with different values of $\\lambda$. How does it change? How does the training error change? Do this in the next cell: **plot a graph showing on the x-axis different values of lambda and on the y-axis the corresponding train and test errors for that $\\lambda$. Comment on any interesting pattern you onbserve.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fill here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Ivanov Regularization\n",
    "\n",
    "Another regularization strategy we considered in class was Ivanov regularization. This consists in finding the best $w$ **within a ball of fixed radius $\\gamma$** fitting the empirical risk. Formally:\n",
    "\n",
    "$\\qquad\\qquad w_\\gamma = \\arg\\min_{\\|w\\|\\leq \\gamma}~ \\mathcal{E}(w)$\n",
    "\n",
    "#### **Problem 4** Implement Ivanov Regularization\n",
    "\n",
    "Ivanov regularization is an example of __constrained optimization__, namely optimization of a function over a smaller set than the entire $\\mathcal{R}^d$. We cannot use gradient descent directly, since gradient desent does not have any way to control the norm of $w$ and might lead the iterates outside the ball of radius $\\gamma$. However, we have discussed in class that we can modify gradient descent to **projected** gradient descent, by adding a small routine at the end of each step, projecting back to the ball of radius $\\eta$.\n",
    "\n",
    "In other words, projected gradient descent works as follows:\n",
    "- choose an initial point $w_0$ (for example $w_0 = 0$).\n",
    "- for any time step $t>0$ compute $u_t$ as\n",
    "$\\qquad u_t = w_{t-1} - \\eta \\nabla \\mathcal{E}(w_{t-1})$,\n",
    "- obtain\n",
    "$\\qquad\\qquad\\qquad w_t = \\left\\{ \\begin{matrix} u_t & \\textrm{if}\\quad \\|u_t\\|\\leq \\gamma \\\\ u_t/\\|u_t\\| & \\textrm{otherwise}\\end{matrix}\\right.$,\n",
    "In other words, keep $u_t$ as it is if it is already in the ball or project it on the surface of the ball if it is outside.\n",
    "\n",
    "Equipped with this variant of gradient descent, proceed in implementing Ivanov regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "T = 1000\n",
    "eta = # how to choose it?\n",
    "lam = # how to choose it?\n",
    "\n",
    "w = 0\n",
    "\n",
    "# in this list we will store the function values\n",
    "values = []\n",
    "\n",
    "\n",
    "# let's start by putting the first value in\n",
    "values.append(empirical_risk(w))\n",
    "\n",
    "# implement gradient descent\n",
    "for t in range(T):\n",
    "    grad_w = # compute the gradient of the empirical risk\n",
    "    u = # write the update for u\n",
    "    w = # write the projection for u to w\n",
    "\n",
    "    # compute the empirical risk of the current iterate and store it\n",
    "    values.append(empirical_risk(w))\n",
    "\n",
    "\n",
    "plt.plot(values)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "How does the text error changes with different values $\\gamma$? To complete the problem repeat the same analysis you did for Tikhonov, plotting train and test error (y-axies) against different values of $\\gamma$ (x-axis). Commenton any interesting pattern you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fill here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Computational Regularization (AKA Early stopping)\n",
    "\n",
    "Finally we will review one of the strategies we discussed in class: early stopping. As the name suggests, this strategy consisst simply in not performing \"too many\" steps of gradient descent, but rather \"stopping early\". How early? Well, the number of steps done becomes the regularization parameter. Too many and we might overfit, too few and we are not learning enough from our data.\n",
    "\n",
    "\n",
    "#### **Problem 5**: Implement Early Stopping\n",
    "\n",
    "Interestingly enough, you have already implemented this strategy in Problem 2 when you optimized the empirical risk without any penalty or constraint. You simply have to play with the regularization parameter `T` to see what happens. To complete this problem repeat the same analysis you have done for Tikhonov and Ivanov: plot the train and test errors with respect to the corresponding values of the regularization parameter (in this case the number of total steps `T`). Comment on any interesting pattern you observe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fill here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "You have reached the end of this exam. Congratulations!\n",
    "\n",
    "To complete the exam send the filled notebook to the email **`c` dot `ciliberto` at `ucl` dot `ac` dot `uk`** with the subject \"(Master MDA)(<your-name>-<your-surname>).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('master-data-analytics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "6036adac460740e43132ebf835365b0c00650083cefb4f7f8a0f3d025846cffa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
