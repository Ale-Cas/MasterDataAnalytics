{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent: implementazione ed esecuzione su vari esempi\n",
    "\n",
    "<table align=\"center\">\n",
    "<tr>\n",
    "    <td>\n",
    "<img src=\"img/gd_surface.png\" width=\"100%\">\n",
    "    </td>\n",
    "    <td>\n",
    "<img src=\"img/gd_levels.png\" width=\"45%\">\n",
    "    </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "$$ w^{(t+1)} = w^{(t)} - \\eta \\, \\nabla g(w^{(t)}), \\qquad t=1,2,\\ldots,T $$\n",
    "\n",
    "In questa esercitazione implementiamo l'algoritmo Gradient Descent in NumPy e lo applichiamo su vari semplici esempi di funzioni, sia convesse che non convesse. \n",
    "\n",
    "Iniziamo caricando NumPy (e MatPlotLib per i grafici)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio 0: Completare l'implementazione di GD\n",
    "\n",
    "La seguente funzione `gradient_descent` contiene una implementazione quasi completa dell'algoritmo, ma manca proprio del passo fondamentale di aggiornamento del vettore $w$. \n",
    "\n",
    "La funzione riceve in input:\n",
    "\n",
    "* la funzione da minimizzare, $g: R^N \\to R$\n",
    "* la funzione gradiente, $\\nabla g: R^N \\to R^N$\n",
    "* il passo $\\eta > 0$\n",
    "* il numero di iterazioni $T$\n",
    "* il vettore iniziale $w^{(1)}$\n",
    "\n",
    "La funzione restituisce due liste:\n",
    "\n",
    "* `w_history` $ = [ w^{(1)}, w^{(2)}, \\ldots, w^{(T)} ] $ è la sequenza dei vettori $w^{(t)}$ generati dall'algoritmo Gradient Descent\n",
    "* `cost_history` $ = [ g(w^{(1)}), g(w^{(2)}), \\ldots, g(w^{(T)}) ] $ è la sequenza dei corrispondenti valori $g(w^{(t)})$\n",
    "\n",
    "Si studi il codice dell'algoritmo e, sulla base di quanto visto a lezione, lo si completi della riga mancante che effettua l'aggiornamento del vettore $w$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: g (funzione obiettivo), nabla_g (funzione gradiente), \n",
    "# eta (passo), T (numero massimo di iterazioni), w_init (punto iniziale)\n",
    "def gradient_descent(g, nabla_g, eta, T, w_init):\n",
    "    w = w_init            # inizializzazione del vettore dei parametri w\n",
    "    w_history = [w]       # lista contenente l'andamento storico dei parametri\n",
    "    cost_history = [g(w)] # lista contenente i corrispondenti valori della funzione costo\n",
    "    # Ciclo principale\n",
    "    for t in range(T):\n",
    "        # valutazione del gradiente\n",
    "        grad_eval = nabla_g(w)\n",
    "        # applicazione di un passo di discesa del gradiente\n",
    "        ### INSERIRE QUI SOTTO LA RIGA DI CODICE CHE EFFETTUA L'AGGIORNAMENTO DEL VETTORE w \n",
    "        ### COME PREVISTO DALL'ALGORITMO\n",
    "\n",
    "        ###\n",
    "        # salvataggio di parametri e costo\n",
    "        w_history.append(w)\n",
    "        cost_history.append(g(w))\n",
    "    # restituiamo l'intera evoluzione storica di parametri e costo\n",
    "    return w_history, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fare **doppio-clic qui** per la soluzione dell'esercizio. \n",
    "\n",
    "<!--\n",
    "        # applicazione di un passo di discesa del gradiente\n",
    "        w = w - eta*grad_eval\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo anche una funzione ausiliaria, usata per graficare l'andamento storico del costo del vettore $w$ negli esempi successivi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione che grafica l'andamento storico del costo\n",
    "def plot_loss(cost_history):\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoca t\")\n",
    "    plt.ylabel(\"Costo g(w_t)\")\n",
    "\n",
    "    T = len(cost_history)\n",
    "    plt.plot(np.arange(0, T), cost_history, label=\"g(w_t)\")\n",
    "    plt.legend()\n",
    "    plt.ylim([np.min(cost_history) - 0.03*np.abs(np.min(cost_history)), np.max(cost_history)])\n",
    "    plt.show()\n",
    "    print('Ultimo valore di w: ', w_history[-1])\n",
    "    print('Ultimo valore di g(w): ', cost_history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un esempio convesso: $g(w) = (w^4 + w^2 + 10w) / 50$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cominciamo definendo la funzione da minimizzare e il suo gradiente (calcolato analiticamente \"a mano\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g1(w):\n",
    "    return (w**4 + w**2 + 10*w) / 50\n",
    "\n",
    "def nabla_g1(w):\n",
    "    return (4*(w**3) + 2*w + 10) / 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tratta di una funzione convessa: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafichiamo g(w)\n",
    "plt.plot(np.arange(-5, 5, 0.01), g1(np.arange(-5, 5, 0.01)), label='g(w)');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per approssimare il punto di minimo globale della funzione, proviamo ad applicare GD per 1000 iterazioni con passo $\\eta=0.001$ a partire dal punto $w_{init} = 2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_history, cost_history = gradient_descent(g1, nabla_g1, 0.001, 1000, np.array([2.0]))\n",
    "plot_loss(cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Esercizio 1**. Eseguire Gradient Descent con $\\eta = 1.0$, $\\eta = 0.1$, $\\eta = 0.01$ per 1000 iterazioni dal punto iniziale $w_{init} = 2$. Quale tra questi valori di $\\eta$ funziona meglio per questa particolare funzione e punto iniziale? Cosa succede utilizzando invece $\\eta= 10$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un esempio non convesso: $g(w) = \\sin(3w) + 0.3 w^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g2(w):\n",
    "    return np.sin(3*w) + 0.3 * w**2\n",
    "\n",
    "def nabla_g2(w):\n",
    "    return 3*np.cos(3*w) + 0.6 * w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso la funzione non è convessa e presenta più minimi locali: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafichiamo g(w)\n",
    "plt.plot(np.arange(-5, 5, 0.01), g2(np.arange(-5, 5, 0.01)), label='g(w)');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafichiamo il costo ottenuto con $\\eta = 0.05$, $T=10$ partendo da $w_{init} = -1.5$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_history, cost_history = gradient_descent(g2, nabla_g2, 0.05, 10, np.array([-1.5]))\n",
    "plot_loss(cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Esercizio 2**. Graficare il costo ottenuto con $\\eta = 0.05$, $T=10$ partendo da un altro punto iniziale: $w_{init} = 4.5$. Verificare che nonostante ci sia convergenza, il minimo locale raggiunto ha un diverso valore rispetto a quello ottenuto partendo da $-1.5$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un esempio multivariato: $g(w) = w_0^2 + w_1^2 + 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo ultimo esempio lo spazio ambiente $R^N$ è bidimensionale. \n",
    "\n",
    "Notare che ora la funzione gradiente (`nabla_g3`) deve restituire un vettore di lunghezza $2$ (basato sulle derivate parziali). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g3(w):\n",
    "    return w[0]**2 + w[1]**2 + 2\n",
    "\n",
    "def nabla_g3(w):\n",
    "    return np.array([2*w[0], 2*w[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nell'invocazione di `gradient_descent`, l'unica differenza sintattica è che ora il vettore di inizializzazione $w_{init}$ sarà un vettore di lunghezza $2$ anziché $1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_history, cost_history = gradient_descent(g3, nabla_g3, 0.05, 50, np.array([1.8, 2.0]))\n",
    "plot_loss(cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio 3\n",
    "**Esercizio 3**. Determinare (a meno di un errore $\\pm 0.0001$) il valore di $w \\in R$ che minimizza globalmente la funzione convessa $g(w) = 3w^2 + e^{-w}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
